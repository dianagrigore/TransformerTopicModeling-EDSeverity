{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkGzm741D7q5"
      },
      "outputs": [],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset"
      ],
      "metadata": {
        "id": "fCh433TbPvEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "q4ZrOWFmF129"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_posts_from_user(writing):\n",
        "    posts = []\n",
        "    for wrt in writing:\n",
        "        user_post = {}\n",
        "        user_post['TITLE'] = wrt.find('TITLE').text\n",
        "        user_post['DATE'] = wrt.find('DATE').text\n",
        "        user_post['INFO'] = wrt.find('INFO').text\n",
        "        user_post['TEXT'] = wrt.find('TEXT').text\n",
        "        posts.append(user_post)\n",
        "        training_sample = pd.DataFrame(posts)\n",
        "    return training_sample"
      ],
      "metadata": {
        "id": "6jqAqQK5Gsep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_posts_from_user_test(writing):\n",
        "    posts = []\n",
        "    for wrt in writing:\n",
        "        user_post = {}\n",
        "        user_post['TITLE'] = wrt.find('TITLE').text[3:-2]\n",
        "        user_post['DATE'] = wrt.find('DATE').text[3:-2]\n",
        "        user_post['INFO'] = wrt.find('INFO').text\n",
        "        user_post['TEXT'] = wrt.find('TEXT').text[3:-2]\n",
        "        posts.append(user_post)\n",
        "        training_sample = pd.DataFrame(posts)\n",
        "    return training_sample"
      ],
      "metadata": {
        "id": "EThxprVLG15S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "FOLDER = '/content/drive/MyDrive/erisk/data2018/train/positive_examples'\n",
        "train_data = pd.DataFrame([])\n",
        "for i in range(1, 10):\n",
        "    chunk = f'/content/drive/MyDrive/erisk/data2018/train/positive_examples/chunk{i}'\n",
        "    files = os.listdir(chunk)\n",
        "    for file in files:\n",
        "      with open(f'{chunk}/{file}') as fp:\n",
        "          soup = BeautifulSoup(fp, 'xml')\n",
        "      writing = soup.find_all('WRITING')\n",
        "      training_sample = read_posts_from_user(writing)\n",
        "      training_sample['USER'] = f'eRisk2022-T3_Subject{i}'\n",
        "      train_data = pd.concat([train_data, training_sample])"
      ],
      "metadata": {
        "id": "t-_cNKd7G3mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.drop(['TITLE', 'DATE', 'INFO', 'USER'], axis=1)\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "Jyvr05fCG_rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.read_csv('/content/drive/MyDrive/erisk/data2018/test/risk-golden-truth-test.txt', delim_whitespace=True, header=None)"
      ],
      "metadata": {
        "id": "N8Tixrd1KxiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.head()"
      ],
      "metadata": {
        "id": "uy-3bCCHLePa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_subjects = labels[labels[1] == 1][0].values\n",
        "positive_subjects"
      ],
      "metadata": {
        "id": "WJFiRebQLidw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.DataFrame([])\n",
        "for i in range(1, 10):\n",
        "    chunk = f'/content/drive/MyDrive/erisk/data2018/test/Task2_chunk{i}/chunk{i}'\n",
        "    files = os.listdir(chunk)\n",
        "    for file in files:\n",
        "      subject = file.split('.')[0].split('_')[0]\n",
        "      if subject in positive_subjects:\n",
        "        #print(subject)\n",
        "        with open(f'{chunk}/{file}') as fp:\n",
        "            soup = BeautifulSoup(fp, 'xml')\n",
        "        writing = soup.find_all('WRITING')\n",
        "        training_sample = read_posts_from_user(writing)\n",
        "        training_sample['USER'] = f'eRisk2022-T3_Subject{i}'\n",
        "        test_data = pd.concat([test_data, training_sample])"
      ],
      "metadata": {
        "id": "vo7cslQzM3if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data.drop(['TITLE', 'DATE', 'INFO', 'USER'], axis=1)"
      ],
      "metadata": {
        "id": "W1ZEemdaOfIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "k8UKzqU6UpMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat([train_data, test_data], ignore_index=True)"
      ],
      "metadata": {
        "id": "hxu8AjpyUubO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "all_data['TEXT'] = [data.strip() for data in all_data['TEXT']]\n",
        "all_data['TEXT'] = [np.nan if data == '' else data for data in all_data['TEXT']]\n",
        "all_data = all_data.dropna()"
      ],
      "metadata": {
        "id": "h0g-DIinI-f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data"
      ],
      "metadata": {
        "id": "RqvoodGjVEtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "PJ4QXsoWsPSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data_txt = all_data.to_csv('all_data.csv', header=None, index=None, sep=' ')"
      ],
      "metadata": {
        "id": "HsPpY7pzs4BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data_txt = all_data.to_csv('all_data.txt', header=None, index=None, sep=' ')"
      ],
      "metadata": {
        "id": "s0cNbxgZiuUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSrQHt6gvYnh",
        "outputId": "fbc87251-928b-4c67-9071-794f0e856a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('all_data.txt', 'a') as f:\n",
        "    df_string = all_data['TEXT'].to_string(header=False, index=False)\n",
        "    f.write(df_string)"
      ],
      "metadata": {
        "id": "YuABioTvfYcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This file loads sentences from a provided text file. It is expected, that the there is one sentence per line in that text file.\n",
        "TSDAE will be training using these sentences. Checkpoints are stored every 500 steps to the output folder.\n",
        "Usage:\n",
        "python train_tsdae_from_file.py path/to/sentences.txt\n",
        "\"\"\"\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
        "from sentence_transformers import models, datasets, losses\n",
        "import logging\n",
        "import gzip\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import tqdm\n",
        "\n",
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "#### /print debug information to stdout\n",
        "\n",
        "# Train Parameters\n",
        "model_name = 'distilroberta-base'\n",
        "batch_size = 8\n",
        "\n",
        "#Input file path (a text file, each line a sentence)\n",
        "if len(sys.argv) < 2:\n",
        "    print(\"Run this script with: python {} path/to/sentences.txt\".format(sys.argv[0]))\n",
        "    exit()\n",
        "\n",
        "filepath = 'all_data.txt'\n",
        "\n",
        "# Save path to store our model\n",
        "output_name = ''\n",
        "if len(sys.argv) >= 3:\n",
        "    output_name = \"-\"+sys.argv[2].replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "\n",
        "#model_output_path = 'output/train_tsdae{}-{}'.format(output_name, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "model_output_path = '/content/drive/MyDrive/erisk/output_roberta/train_tsdae{}-{}'.format(output_name, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
        "\n",
        "\n",
        "################# Read the train corpus  #################\n",
        "train_sentences = []\n",
        "with gzip.open(filepath, 'rt', encoding='utf8') if filepath.endswith('.gz') else open(filepath, encoding='utf8') as fIn:\n",
        "    for line in tqdm.tqdm(fIn, desc='Read file'):\n",
        "        line = line.strip()\n",
        "        if len(line) >= 10:\n",
        "            train_sentences.append(line)\n",
        "\n",
        "\n",
        "logging.info(\"{} train sentences\".format(len(train_sentences)))\n",
        "\n",
        "################# Intialize an SBERT model #################\n",
        "\n",
        "word_embedding_model = models.Transformer(model_name)\n",
        "# Apply **cls** pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), 'cls')\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "################# Train and evaluate the model (it needs about 1 hour for one epoch of AskUbuntu) #################\n",
        "# We wrap our training sentences in the DenoisingAutoEncoderDataset to add deletion noise on the fly\n",
        "train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "train_loss = losses.DenoisingAutoEncoderLoss(model, decoder_name_or_path=model_name, tie_encoder_decoder=True)\n",
        "\n",
        "\n",
        "logging.info(\"Start training\")\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=20,\n",
        "    weight_decay=0,\n",
        "    scheduler='constantlr',\n",
        "    optimizer_params={'lr': 3e-5},\n",
        "    show_progress_bar=True,\n",
        "    checkpoint_path=model_output_path,\n",
        "    use_amp=False                #Set to True, if your GPU supports FP16 cores\n",
        ")"
      ],
      "metadata": {
        "id": "y6NfZtt1vAa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "vbQz3yteNHEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This file runs Masked Language Model. You provide a training file. Each line is interpreted as a sentence / paragraph.\n",
        "Optionally, you can also provide a dev file.\n",
        "The fine-tuned model is stored in the output/model_name folder.\n",
        "Usage:\n",
        "python train_mlm.py model_name data/train_sentences.txt [data/dev_sentences.txt]\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import sys\n",
        "import gzip\n",
        "from datetime import datetime\n",
        "\n",
        "if len(sys.argv) < 3:\n",
        "    print(\"Usage: python train_mlm.py model_name data/train_sentences.txt [data/dev_sentences.txt]\")\n",
        "    exit()\n",
        "\n",
        "model_name = 'mental/mental-bert-base-uncased'\n",
        "per_device_train_batch_size = 64\n",
        "\n",
        "save_steps = 1000               #Save model every 1k steps\n",
        "num_train_epochs = 1           #Number of epochs\n",
        "use_fp16 = False                #Set to True, if your GPU supports FP16 operations\n",
        "max_length = 100                #Max length for a text input\n",
        "do_whole_word_mask = True       #If set to true, whole words are masked\n",
        "mlm_prob = 0.15                 #Probability that a word is replaced by a [MASK] token\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name, use_auth_token='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')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token='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')\n",
        "\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/erisk/output_mhb1_epochs\"\n",
        "print(\"Save checkpoints to:\", output_dir)\n",
        "\n",
        "\n",
        "##### Load our training datasets\n",
        "\n",
        "train_sentences = []\n",
        "train_path = 'all_data.txt'\n",
        "with gzip.open(train_path, 'rt', encoding='utf8') if train_path.endswith('.gz') else  open(train_path, 'r', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        line = line.strip()\n",
        "        if len(line) >= 10:\n",
        "            train_sentences.append(line)\n",
        "\n",
        "print(\"Train sentences:\", len(train_sentences))\n",
        "\n",
        "dev_sentences = []\n",
        "if len(sys.argv) >= 4:\n",
        "    dev_path = sys.argv[3]\n",
        "    with gzip.open(dev_path, 'rt', encoding='utf8') if dev_path.endswith('.gz') else open(dev_path, 'r', encoding='utf8') as fIn:\n",
        "        for line in fIn:\n",
        "            line = line.strip()\n",
        "            if len(line) >= 10:\n",
        "                dev_sentences.append(line)\n",
        "\n",
        "print(\"Dev sentences:\", len(dev_sentences))\n",
        "\n",
        "#A dataset wrapper, that tokenizes our data on-the-fly\n",
        "class TokenizedSentencesDataset:\n",
        "    def __init__(self, sentences, tokenizer, max_length, cache_tokenization=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sentences = sentences\n",
        "        self.max_length = max_length\n",
        "        self.cache_tokenization = cache_tokenization\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if not self.cache_tokenization:\n",
        "            return self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
        "\n",
        "        if isinstance(self.sentences[item], str):\n",
        "            self.sentences[item] = self.tokenizer(self.sentences[item], add_special_tokens=True, truncation=True, max_length=self.max_length, return_special_tokens_mask=True)\n",
        "        return self.sentences[item]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "train_dataset = TokenizedSentencesDataset(train_sentences, tokenizer, max_length)\n",
        "dev_dataset = TokenizedSentencesDataset(dev_sentences, tokenizer, max_length, cache_tokenization=True) if len(dev_sentences) > 0 else None\n",
        "\n",
        "\n",
        "##### Training arguments\n",
        "\n",
        "if do_whole_word_mask:\n",
        "    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
        "else:\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=mlm_prob)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    evaluation_strategy=\"steps\" if dev_dataset is not None else \"no\",\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    eval_steps=save_steps,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=save_steps,\n",
        "    save_total_limit=1,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=use_fp16\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset\n",
        ")\n",
        "\n",
        "print(\"Save tokenizer to:\", output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Save model to:\", output_dir)\n",
        "model.save_pretrained(output_dir)\n",
        "\n",
        "print(\"Training done\")"
      ],
      "metadata": {
        "id": "OFpsZpfUSnxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "import torch\n",
        "# /content/output/distilroberta-base-2023-04-07_01-22-09/checkpoint-1000/pytorch_model.bin\n",
        "checkpoint = torch.load('/content/output/distilroberta-base-2023-04-07_01-22-09/checkpoint-1000/pytorch_model.bin')\n",
        "model.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "id": "0kLdUvClae8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.state_dict():\n",
        "  print(layer)"
      ],
      "metadata": {
        "id": "tE6kvp0Xbdp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in checkpoint:\n",
        "  print(layer)"
      ],
      "metadata": {
        "id": "X1h_4a60a_cM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}